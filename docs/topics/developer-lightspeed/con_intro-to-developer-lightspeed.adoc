// Module included in the following assemblies:
//
// * docs/cli-guide/master.adoc

:_content-type: CONCEPT
[id="intro-to-the-developer-lightspeed_{context}"]
= Introduction to the {mta-dl-plugin}

[role="_abstract"]
Starting from 8.0.0, {ProductFullName} integrates with large language models (LLM) through the {mta-dl-full} component in the Visual Studio Code extension. You can use {mta-dl-plugin} to apply LLM-driven code changes to resolve issues found through static code analysis of Java applications.

[id="use-case-ai-code-fix_{context}"]
== Use case for AI-driven code fixes

{ProductFirstRef} performs the static code analysis for a specified target technology to which you want to migrate your applications. Red Hat provides 2400+ analysis rules in {ProductShortName} for many Java technologies and you can extend the ruleset for custom frameworks or new technologies by creating custom rules.

The static code analysis describes the issues in your code that you must resolve. As you perform analysis for a large portfolio of applications, the issue description and the rule definition that might contain extra information form a large corpus of data that contains repetitive patterns of problem definitions and  solutions.

Migrators do duplicate work by resolving issues that repeat across applications in different migration waves.

[id="how-developerlightspped-works_{context}"]
== How does {mta-dl-plugin} work

{mta-dl-plugin} works by collecting and storing the changes in the code for a large collection of applications, finding context to generate prompts for the LLM of your choice, and by generating code resolutions produced by the LLM to address specific issues.

{mta-dl-plugin} uses Retrieval Augmented Generation for context-based resolutions of issues in code. By using RAG, {mta-dl-plugin} improves the context shared with the LLM to generate more accurate suggestions to fix the issue in the code. The context allows the LLM to "reason" and generate suggestions for issues detected in the code. This mechanism helps to overcome the limited context size in LLMs that prevents them from analyzing the entire source code of an application.

The context is a combination of the source code, the issue description, and solved examples: 

* Description of issues detected by {ProductShortName} when you run a static code analysis for a given set of target technologies.

* (Optional) The default and custom rules might contain extra information that you include which can help {mta-dl-plugin} to define the context.
+
* Solved examples constitute code changes from other migrations and a pattern of resolution for an issue that can be used in future. The system creates a solved example when a Migrator accepts a resolution in a previous analysis that results in updated code or an unfamiliar issue in a legacy application that the Migrator manually fixed. The Solution Server stores solved examples. 
+
More instances of solved examples for an issue enhances the context and improve the success metrics of rules that trigger the issue. A higher success metrics of an issue refers to the higher confidence level associated with the accepted resolutions for that issue in previous analyses.

* (Optional) If you enable the Solution Server, it extracts a pattern of resolution, called the migration hint, that can be used by the LLM to generate a more accurate fix suggestion in a future analysis.
+
The improvement in the quality of migration hints results in more accurate code resolutions. Accurate code resolutions from the LLM result in the user accepting an update to the code. The Solution Server stores the updated code to generate a better migration hint in future.
+
This cyclical improvement of resolution pattern from the Solution Server and improved migration hints lead to more reliable code changes as you migrate applications in different migration waves.

[id="modes-developer-lightspeed_{context}"]
== Requesting code fixes in {mta-dl-plugin}

You can request AI-assisted code resolutions that get context from multiple potential sources, such as analysis issues, IDE diagnostic information, and past migration data via the Solution Server.

The Solution Server acts as an institutional memory that stores changes to source codes after analyzing applications in your organization. This helps you use the recurring patterns of solutions for issues that repeat in many applications.

When you use the Solution Server, {mta-dl-plugin} suggests a code resolution based on solved examples or code changes in past analysis. You can view a diff of the updated portions of the code and the original source code to do a manual review. 

It also enables you to control the analysis through manual reviews of the suggested AI resolutions: you can accept, reject or edit the suggested code changes while reducing the time and effort required to prepare your application for migration.

In the agentic AI mode, {mta-dl-plugin} streams an automated analysis of the code in a loop until it resolves all issues and changes the code with the updates. In the initial run, the AI agent:

* Plans the context to define the issues.
* Chooses a suitable sub agent for the analysis task.
Works with the LLM to generate fix suggestions. The tool displays the reasoning transcript and files to be changed to the user.
* Applies the changes to the code once the user approves the updates.

If you accept that the agentic AI must continue to make changes, it compiles the code and runs a partial analysis. In this iteration, the agentic AI attempts to fix diagnostic issues (if any) generated by tools that you installed in the Visual Studio Code IDE. You can review the changes and accept the agentic AI's suggestion to address these diagnostic issues. 

After each iteration of applying changes to the code, the agentic AI asks if you want the agent to continue fixing more issue. When you accept, it runs another iteration of automated analysis until it has resolved all issues or it has made up to two attempts to fix an issue.

Agentic AI generates a new preview in each iteration when it updates the code with the suggested resolutions. The time taken by the agentic AI to complete all iterations depends on the number of new diagnostic issues that the tool detects in the code.

//You can consider using the demo mode for running {mta-dl-plugin} when you need to perform analysis but have a limited network connection for {mta-dl-plugin} to sync with the LLM. The demo mode stores the input data as a hash and past LLM calls in a cache. The cache is stored in a chosen location in the your file system for later use. The hash of the inputs is used to determine which LLM call must be used in the demo mode. After you enable the demo mode and configure the path to your cached LLM calls in the {mta-dl-plugin} settings, you can rerun an analysis for the same set of files using the responses to a previous LLM call.

:FeatureName: Developer Lightspeed for MTA

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
:!FeatureName:

[id="benefits-using-developer-lightspeed_{context}"]
== Benefits of using {mta-dl-plugin}

* *Model agnostic* - {mta-dl-plugin} follows a "Bring Your Own Model" approach, allowing your organization to use a preferred LLM.
* *Iterative refinement* - {mta-dl-plugin} can include an agent that iterates through the source code to run a series of automated analyses that resolves both the code base and diagnostic issues.
* *Contextual code generation* - By leveraging AI for static code analysis, {mta-dl-plugin} breaks down complex problems into more manageable ones, providing the LLM with focused context to generate meaningful results. This helps overcome the limited context size of LLMs when dealing with large codebases.
* *No fine tuning* - You also do not need to fine tune your model with a suitable data set for analysis which leaves you free to use and switch LLM models to respond to your requirements.
* *Learning and Improvement* - As you migrate more parts of a codebase with {mta-dl-plugin}, it can use RAG to learn from the available data and provide better recommendations in an application analysis.

