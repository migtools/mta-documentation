// Module included in the following assemblies:
//
// * docs/cli-guide/master.adoc

:_content-type: CONCEPT
[id="intro-to-the-developer-lightspeed_{context}"]
= Introduction to the {mta-dl-plugin}

Starting from 8.0.0, {ProductFullName} integrates with large language models (LLM) through the {mta-dl-full} component in the Visual Studio (VS) Code extension. You can use {mta-dl-plugin} to apply LLM-driven code changes to resolve issues found through static code analysis of Java applications.    

[id="use-case-ai-code-fix_{context}"]
== Use case for AI-driven code fixes

{ProductShortName} performs the static code analysis for a specified target technology to which you want to migrate your applications. Red Hat provides 2400+ analysis rules in {ProductShortName} for various Java technologies and you can extend the ruleset for custom frameworks or new technologies by creating custom rules. 

The static code analysis identifies issues in your code that must be resolved. When working with a large portfolio of applications, the issue descriptions and rule definitions generate a large corpus of repetitive problem statements and solutions. As a result, migrators often duplicate work by resolving the same issues across multiple applications and migration waves.

Migrators do duplicate work by resolving issues that are repeated across applications in different migration waves.

[id="how-developerlightspped-works_{context}"]
== How does {mta-dl-plugin} work

{mta-dl-plugin} works by collecting and storing code changes across many applications, generating context to create prompts for the LLM of your choice, and producing migration hints from the LLM to resolve specific issues.

The LLM generates migration hints based on the context shared by the {mt-dl-plugin}.
The context allows the LLM to "reason" and generate the hints. This implementation helps to overcome the limited context size that prevents LLMs from analyzing the source code of an entire application and knowledge gap which often lack organization-specific details because they are not trained on that data. 

The context is a combination of the following inputs that are shared with the LLM:

* Description of issues detected by {ProductShortName} when you run a static code analysis for a given set of target technologies. 

* (Optional) Extra information that you include in the rules. The default and custom rules may contain additional information that helps {mta-dl-plugin} to define the context. 
+
* When you enable the Solution Server, a solved example is created when a Migrator accepts a resolution in a previous analysis that results in updated code or an unfamiliar issue in a legacy application that the Migrator manually fixed. Solved examples are stored in the Solution Server. 
+
More instances of solved examples for an issue enhance the context and improves the success metrics of rules that trigger the issue. A higher success metrics of an issue refers to the higher confidence level associated with the accepted resolutions for that issue in previous analyses.

* (Optional) If you enable the solution server mode, the Solution Server extracts a pattern of solution that can be used by the LLM to generate a more accurate migration hint. 
+
The improvement in the quality of migration hints results in more accurate code resolutions. In turn, these updated code is stored in the solution server to generate a better migration hint in future. 
+
This cyclical improvement of resolution pattern from the solution server and improved migration hints lead to more reliable code changes as you migrate applications in different migration waves

The Solution Server acts as institutional memory capturing code changes from previous migrations. This helps you to leverage the recurring patterns of solutions for issues that are repeated in many applications. 

Thus, when you deploy {mta-dl-plugin} for analyzing your entire application portfolio, it enables you to be consistent with the common fixes you need to make in the source code of any Java application. 

It also enables you to control the analysis through manual reviews of the suggested AI resolutions by accepting or rejecting the changes while reducing the overall time and effort required to prepare your application for migration.

[id="modes-developer-lightspeed_{context}"]
== Modes in {mta-dl-plugin}

You can run an analysis for AI-assisted code fixes in two modes: the Agentic AI and the Retrieval Augmented Generation (RAG) solution delivered by the Solution Server.

If you enable the agentic AI mode, {mta-dl-plugin} streams an automated analysis of the code in a loop until all issues are resolved and changes the code with the updates. In the initial run, the AI agent:

* Plans the context to define the issues. 
* Chooses a suitable sub agent for the analysis task.
Works with the LLM to generate fix suggestions. The reasoning transcript and files to be changed are displayed to the user.
* Applies the changes to the code once the user approves the updates.

If you accept that the agentic AI must continue to make changes, it compiles the code and runs a partial analysis. In this phase, the agentic AI can detect diagnostic issues (if any) generated by tools that you installed in the VS Code IDE. You can accept the agentic AI's suggestion to address these diagnostic issues too. After every phase of applying changes to the code, the agentic AI runs another round of automated analysis depending on your acceptance, until it has run through all the files in your project and resolved the issues in the code. 

Agentic AI generates a new file in each round when it applies the suggestions in the code. The time taken by the agentic AI to complete several rounds of analysis depends on the size of the application, the number of issues, and the complexity of the code.

When you use the Solution Server mode, {mta-dl-plugin} delivers a solution for an issue that is based on solved examples or code changes in past analysis. When you fix code, you can view a diff of the updated portions of the code and the original source code to do a manual review. In such an analysis, the user has more control over the changes that must be applied to the code. 

//You can consider using the demo mode for running {mta-dl-plugin} when you need to perform analysis but have a limited network connection for {mta-dl-plugin} to sync with the LLM. The demo mode stores the input data as a hash and past LLM calls in a cache. The cache is stored in a chosen location in the your file system for later use. The hash of the inputs is used to determine which LLM call must be used in the demo mode. After you enable the demo mode and configure the path to your cached LLM calls in the {mta-dl-plugin} settings, you can rerun an analysis for the same set of files using the responses to a previous LLM call.

[id="benefits-using-developer-lightspeed_{context}"]
== Benefits of using {mta-dl-plugin}

* *Model agnostic* - {mta-dl-plugin} follows a "Bring Your Own Model" approach, allowing your organization to use a preferred LLM.
* *Iterative refinement* - {mta-dl-plugin} can include an agent that iterates through the source code to run a series of automated analyses that resolves both the code base and diagnostic issues.
* *Contextual code generation* - By leveraging AI for static code analysis, {mta-dl-plugin} breaks down complex problems into more manageable ones, providing the LLM with focused context to generate meaningful results. This helps overcome the limited context size of LLMs when dealing with large codebases.
* *No fine tuning* - You also do not need to fine tune your model with a suitable data set for analysis which leaves you free to use and switch LLM models to respond to your requirements.
* *Learning and Improvement* - As more parts of a codebase are migrated with {mta-dl-plugin}, it can use RAG to learn from the available data and provide better recommendations in subsequent application analysis.
