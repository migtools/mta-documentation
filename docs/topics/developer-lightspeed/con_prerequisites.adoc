:_newdoc-version: 2.15.0
:_template-generated: 2024-2-21

:_mod-docs-content-type: CONCEPT

[id="prerequisites_{context}"]
= Prerequisites

[role="_abstract"]
This section lists the prerequisites required to successfully use the generative AI features in the {mta-dl-plugin} Visual Studio Code extension.

Before you install {mta-dl-plugin}, you must:

* Install Language Support for Java(TM) by Red Hat extension

* Install Java v17 and later

* Install Maven v3.9.9 or later

* Install Git and add it to the $PATH variable

* Install the {ProductShortName} Operator 8.0.0
+

The {ProductShortName} Operator is mandatory if you plan to enable the Solution Server that works with the large language model (LLM) for generating code changes. It enables you to log in to the `openshift-mta` project where you must enable the Solution Server in the Tackle custom resources (CR).

* Create an API key for an LLM.
+

You must enter the provider value and model name in Tackle CR to enable generative AI configuration in the {ProductShortName} Visual Studio Code plugin. 
+
.Configurable large language models and providers
|===
| LLM Provider (Tackle CR value) | Large language model examples for Tackle CR configuration

| {ocp-name} AI platform| Models deployed in an {ocp-name} AI cluster that can be accessed by using Open AI-compatible API.
| Open AI (`openai`) | `gpt-4`, `gpt-4o`, `gpt-4o-mini`, `gpt-3.5-turbo` 
| Azure OpenAI (`azure_openai`) | `gpt-4`, `gpt-35-turbo` 
| Amazon Bedrock (`bedrock`) | `anthropic.claude-3-5-sonnet-20241022-v2:0`, `meta.llama3-1-70b-instruct-v1:0` 
| Google Gemini (`google`) | `gemini-2.0-flash-exp`, `gemini-1.5-pro` 
| Ollama (`ollama`) | `llama3.1`, `codellama`, `mistral` 

|===

[NOTE]
====
The availability of public LLM models is maintained by each LLM provider.
====