// Module included in the following assemblies:
//
// * docs/developer-lightspeed-guide/master.adoc (via assembly_solution-server-configurations.adoc)

:_mod-docs-content-type: CONCEPT
[id="solution-server-overview_{context}"]
= Solution Server overview

[role="_abstract"]
Solution Server builds a collective memory of source code changes from analyses and improves LLM code fix suggestions with contextual hints and success metrics. This topic describes benefits, Technology Preview scope, and supported LLM providers.

[id="solution-server-benefits_{context}"]
== Solution Server benefits

Solution Server is a component that allows {mta-dl-plugin} to build a collective memory of source code changes from all analysis performed in an organization. When you request code fix for issues in Visual Studio Code, the Solution Server augments previous patterns of how source code changed to resolve issues (solved examples) that were similar to those in the current file, and suggests a resolution that has a higher confidence level derived from previous solutions. After you accept a suggested code fix, the Solution Server works with the large language model (LLM) to improve the hints about the issue that becomes part of the context. An improved context enables the LLM to generate more reliable code fix suggestions in future cases.

The Solution Server delivers two primary benefits to users:

* *Contextual Hints*: It surfaces examples of past migration solutions, including successful user modifications and accepted fixes, offering actionable hints for difficult or previously unsolved migration problems.
* *Migration Success Metrics*: It exposes detailed success metrics for each migration rule, derived from real-world usage data. IDEs or automation tools can use these metrics to present users with a "confidence level" or likelihood of {mta-dl-plugin} successfully migrating a given code segment.

Solution Server is an optional component in {mta-dl-plugin}. You must complete the following configurations before you can place a code resolution request.

:FeatureName: Solution Server
[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
:!FeatureName:

[id="solution-server-llm-providers_{context}"]
== Configurable large language models and providers in the Tackle custom resource

.Configurable large language models and providers in the Tackle custom resource
|===
| LLM Provider (Tackle CR value) | Large language model examples for Tackle CR configuration

|{ocp-name} AI platform| Models deployed in an OpenShift AI cluster that can be accessed by using Open AI-compatible API
| Open AI (`openai`) | `gpt-4`, `gpt-4o`, `gpt-4o-mini`, `gpt-3.5-turbo`
| Azure OpenAI (`azure_openai`) | `gpt-4`, `gpt-35-turbo`
| Amazon Bedrock (`bedrock`) | `anthropic.claude-3-5-sonnet-20241022-v2:0`, `meta.llama3-1-70b-instruct-v1:0`
| Google Gemini (`google`) | `gemini-2.0-flash-exp`, `gemini-1.5-pro`
| Ollama (`ollama`) | `llama3.1`, `codellama`, `mistral`

|===
