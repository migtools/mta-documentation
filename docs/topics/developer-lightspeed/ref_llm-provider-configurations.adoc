:_newdoc-version: 2.15.0
:_template-generated: 2024-2-21

:_mod-docs-content-type: REFERENCE

[id="llm-provider-settings_{context}"]
= Provider settings configuration

[role="_abstract"]
{mta-dl-full} is large language model (LLM) agnostic and intergrates with an LLM of your choice. 

To enable {mta-dl-full} to access your large language model (LLM), you must enter the LLM provider configurations in the `provider-settings.yaml` file. 

The `provider-settings.yaml` file contains a list of LLM providers that are suppored by default. The mandatory environment variables are different for each LLM provider. Depending on the provider that you choose, you can configure additional environment variables for a model in the `provider-settings.yaml` file. You can also enter a new provider with the required environment variables, the base URL, and the model name.

The provider settings file is available in the {mta-dl-plugin} Visual Studio (VS) Code extension.

Access the `provider-settings.yaml` from the VS Code Command Pallete by typing `Open the GenAI model provider configuration file`. 

You can select one provider from the list by using the `&active` anchor in the name of the provider. To use a model from another provider, move the `&active` anchor to the desired provider block and restart the solution server on the `Open {ProductShortName} Analysis View` screen.

For an OpenAI model:

[source, yaml]
----
OpenAI: &active
    environment:
      OPENAI_API_KEY: "<your-API-key>" # Required
    provider: ChatOpenAI
    args:
      model: gpt-4o # Required
----

For Azure OpenAI:

[source, yaml]
----
AzureChatOpenAI:
    environment:
      AZURE_OPENAI_API_KEY: "" # Required
    provider: AzureChatOpenAI
    args:
      azureOpenAIApiDeploymentName: "" # Required
      azureOpenAIApiVersion: "" # Required
----

For Amazon Bedrock:

[source, yaml]
----
AmazonBedrock:
    environment:
      ## May have to use if no global `~/.aws/credentials`
      AWS_DEFAULT_REGION: us-east-1
      AWS_ACCESS_KEY_ID: "" # Required if a global ~/.aws/credentials file is not present
      AWS_SECRET_ACCESS_KEY: "" # Required if a global ~/.aws/credentials file is not present
      AWS_DEFAULT_REGION: "" # Required
    provider: ChatBedrock
    args:
      model: meta.llama3-70b-instruct-v1:0 # Required
----

[NOTE]
====
It is recommended to use the link:https://aws.amazon.com/cli/[AWS CLI] and verify that you have command line access to AWS services before you proceed with the `provider-settings` configurations.
====


For Google Gemini:

[source, yaml]
----
GoogleGenAI:
    environment:
      GOOGLE_API_KEY: "" # Required
    provider: ChatGoogleGenerativeAI
    args:
      model: gemini-2.5-pro # Required
----

For Ollama:

[source, yaml]
----
models:
  ChatOllama: 
    provider: "ChatOllama"
    args:
      model: "granite-code:8b-instruct"
      baseUrl: "127.0.0.1:11434" # example URL
----

For a model named "my-model" deployed in {ocp-short} AI with "example-model" as the serving name:

//check if openshift prefix is required for OpenShift AI model provider, like "openshift-example-model" or can it be just "example-model"
[source, yaml]
----
models:
  openshift-example-model: 
    environment:
      CA_BUNDLE: "<Servers CA Bundle path>"
      ALLOW_INSECURE: "true"
    provider: "ChatOpenAI"
    args:
      model: "my-model"
      configuration:
        base_url: "https://<serving-name>-<data-science-project-name>.apps.konveyor-ai.migration.redhat.com/v1"
----

[NOTE]
====
When you change the `model` deployed in {ocp-short} AI, you must also change the `base_url` endpoint.
====