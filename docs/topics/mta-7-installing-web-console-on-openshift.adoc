// Module included in the following assemblies:
//
// * docs/web-console-guide/master.adoc

:_mod-docs-content-type: PROCEDURE
[id="mta-7-installing-web-console-on-openshift_{context}"]

= Installing the {ProductName} {WebName}

You can install the {ProductName} ({ProductShortName}) {WebName} on all Red Hat OpenShift cloud services and Red Hat OpenShift self-managed editions.

[IMPORTANT]
====
To be able to create {ProductShortName} instances, you must first install the {ProductShortName} Operator.
====

The {ProductShortName} Operator is a structural layer that manages resources deployed on OpenShift, such as database, front end, and back end, to automatically create an {ProductShortName} instance.

[id="openshift-persistent-volume-requirements_{context}"]
== Persistent volume requirements

To successfully deploy, the {ProductShortName} Operator requires 2 RWO persistent volumes (PVs) used by different components. If the `rwx_supported` configuration option is set to `true`, the {ProductShortName} Operator requires an additional 2 RWX PVs that are used by Maven and the hub file storage. The PVs are described in the following table:

.Required persistent volumes
[cols="25%,25%,25%,25%", options="header"]
|====
|Name
|Default size
|Access mode
|Description

|`hub database`
|`10Gi`
|RWO
|Hub database

|`hub bucket`
|`100Gi`
|RWX
|Hub file storage; required if the `rwx_supported` configuration option is set to `true`

|`keycloak postgresql`
|`1Gi`
|RWO
|Keycloak back end database

|`cache`
|`100Gi`
|RWX
|Maven m2 cache; required if the `rwx_supported` configuration option is set to `true`

|`{mta-dl-plugin} database`
|`5Gi`
|`RWO`
|{mta-dl-plugin} database required to run an AI-driven analysis.
//verify the final d/s name of this resource.
|====

[id="installing-mta-operator-and-ui_{context}"]
== Installing the {ProductName} Operator and the {WebName}

You can install the {ProductName} ({ProductShortName}) and the {WebName} on Red Hat OpenShift versions {ocp-supported}.

.Prerequisites

* 4 vCPUs, 8 GB RAM, and 40 GB persistent storage.
* Any cloud services or self-hosted edition of Red Hat OpenShift on versions {ocp-supported}.
* You must be logged in as a user with `cluster-admin` permissions.

For more information, see link:https://access.redhat.com/support/policy/updates/openshift_operators[OpenShift Operator Life Cycles].

.Procedure

. In the Red Hat OpenShift web console, click *Operators → OperatorHub*.
. Use the *Filter by keyword* field to search for *MTA*.
. Click the *Migration Toolkit for Applications* Operator and then click *Install*.
. On the *Install Operator* page, click *Install*.
. Click *Operators → Installed Operators* to verify that the {ProductShortName} Operator appears in the `openshift-mta` project with the status `Succeeded`.
. Click the *{ProductShortName}* Operator.
. Under *Provided APIs*, locate *Tackle*, and click *Create Instance*.
+
The *Create Tackle* window opens in *Form* view.
. Review the custom resource (CR) settings. The default choices should be acceptable, but make sure to check the system requirements for storage, memory, and cores.
. To work directly with the YAML file, click *YAML* view and review the CR settings that are listed in the `spec` section of the YAML file.
+
The most commonly used CR settings are listed in this table:
+
.Tackle CR settings
[cols="25%,15%,65%", options="header"]
|====
|Name
|Default
|Description

|`cache_data_volume_size`
|`100Gi`
|Size requested for the cache volume; ignored when `rwx_supported=false`

|`cache_storage_class`
|Default storage class
|Storage class used for the cache volume; ignored when `rwx_supported=false`

|`feature_auth_required`
|`true`
|Flag to indicate whether keycloak authorization is required (single user/"`noauth`")

|`feature_isolate_namespace`
|`true`
|Flag to indicate whether namespace isolation using network policies is enabled

|`hub_database_volume_size`
|`10Gi`
|Size requested for the Hub database volume

|`hub_bucket_volume_size`
|`100Gi`
|Size requested for the Hub bucket volume

|`hub_bucket_storage_class`
|Default storage class
|Storage class used for the bucket volume

|`keycloak_database_data_volume_size`
|`1Gi`
|Size requested for the Keycloak database volume

|`keycloak_sso_req_passwd_update`
|`true`
|If the flag is set to `true`, users are required to update their password after their first login.

|`pathfinder_database_data_volume_size`
|`1Gi`
|Size requested for the Pathfinder database volume

|`maven_data_volume_size`
|`100Gi`
|Size requested for the Maven m2 cache volume; deprecated in {ProductShortName} 6.0.1

|`disable_maven_search`
|`false`
|Set the flag to `true` to disable {ProductShortName} from relying on the Maven search index to determine if a dependency is publicly available (such as an open-source dependency) or internal to the Java binary application during analysis. 

When you disable Maven search, {ProductShortName} at first tries to determine dependencies from the JAR file's POM file (if any). If this method does not succeed, {ProductShortName} goes through the directory structure to determine dependencies. This method may not produce a reliable dependency classification since the package structure can differ from what is expected by {ProductShortName}. You may see more incidents because some dependencies may be wrongly classified as internal.

By default, `disable_maven_search` is set to `false`. Therefore, {ProductShortName} uses the SHA digest of the JAR file to search the Maven search index. This setting generates more accurate dependencies, but the drawback is that the Maven search index is frequently unavailable.

|`rwx_storage_class`
|NA
|Storage class requested for the Tackle RWX volumes; deprecated in {ProductShortName} 6.0.1

|`rwx_supported`
|`true`
|Flag to indicate whether the cluster storage supports RWX mode

|`rwo_storage_class`
|NA
|Storage class requested for the Tackle RW0 volumes

|`analyzer_container_limits_cpu`
|`1`
|Maximum number of CPUs the pod is allowed to use

|`analyzer_container_limits_memory`
|`1Gi`
|Maximum amount of memory the pod is allowed to use. You can increase this limit if the pod displays `OOMKilled` errors.

|`analyzer_container_requests_cpu`
|`1`
|Minimum number of CPUs the pod needs to run

|`analyzer_container_requests_memory`
|`512Mi`
|Minimum amount of memory the pod needs to run

|`ui_container_limits_cpu`
|`500m`
|Maximum number of CPUs the UI pod resource is allowed to use

|`ui_container_limits_memory`
|`800Mi`
|Maximum amount of memory the UI pod resource is allowed to use. You can increase this limit if the pod displays `OOMKilled` errors.

|`ui_container_requests_cpu`
|`100m`
|Minimum number of CPUs the UI pod resource needs to run

|`ui_container_requests_memory`
|`350Mi`
|Minimum amount of memory the UI pod resource needs to run

|`provider_java_container_limits_cpu`
|`1`
|Maximum number of CPUs the Java provider resource is allowed to use

|`provider_java_container_limits_memory`
|`2.5Gi`
|Maximum amount of memory the Java provider resource is allowed to use. You can increase this limit if the pod displays `OOMKilled` errors.

|`provider_java_container_requests_cpu`
|`1`
|Minimum number of CPUs the Java provider resource needs to run

|`provider_java_container_requests_memory`
|`2.5Gi`
|Minimum amount of memory the Java provider resource needs to run
|====

+
.Example YAML file

[source,YAML]
----
kind: Tackle
apiVersion: tackle.konveyor.io/v1alpha1
metadata:
  name: mta
  namespace: openshift-mta
spec:
  hub_bucket_volume_size: "25Gi"
  maven_data_volume_size: "25Gi"
  rwx_supported: "false"
----

. Edit the CR settings if needed, and then click *Create*.
. In *Administration* view, click *Workloads -> Pods* to verify that the MTA pods are running.
. Access the {WebName} from your browser by using the route exposed by the `{LC_PSN}-ui` application within OpenShift.
. Use the following credentials to log in:
** *User name*: admin
** *Password*: Passw0rd!
. When prompted, create a new password.

////
[id="installing-mta-operator-in-disconnected-environment_{context}"]
== Installing the {ProductName} Operator in a disconnected Red Hat OpenShift environment

You can install the {ProductShortName} Operator in a disconnected environment by following the instructions in link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.15/html/installing/disconnected-installation-mirroring#installing-mirroring-disconnected[generic procedure].

In step 1 of the generic procedure, configure the image set for mirroring as follows:

[source,yaml]
----
kind: ImageSetConfiguration
apiVersion: mirror.openshift.io/v1alpha2
storageConfig:
  registry:
    imageURL: registry.to.mirror.to
    skipTLS: false
mirror:
  operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.15
    packages:
    - name: mta-operator
      channels:
      - name: stable-v7.0
    - name: rhsso-operator
      channels:
      - name: stable
  helm: {}
----
////

[id="eviction-threshold_{context}"]
=== Eviction threshold

Each node has a certain amount of memory allocated to it. Some of that memory is reserved for system services. The rest of the memory is intended for running pods. If the pods use more than their allocated amount of memory, an out-of-memory event is triggered and the node is terminated with a `OOMKilled` error.

To prevent out-of-memory events and protect nodes, use the `--eviction-hard` setting. This setting specifies the threshold of memory availability below which the node evicts pods. The value of the setting can be absolute or a percentage.

.Example of node memory allocation settings

- Node capacity: `32Gi`

- `--system-reserved` setting: `3Gi`

- `--eviction-hard` setting: `100Mi`

The amount of memory available for running pods on this node is 28.9 GB. This amount is calculated by subtracting the `system-reserved` and `eviction-hard` values from the overall capacity of the node. If the memory usage exceeds this amount, the node starts evicting pods.


[id="mta-7-red-hat-build-of-keycloak_{context}"]
== Red Hat Build of Keycloak

The {ProductShortName} 7.3.0 uses link:https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.0[{rhbk-first}] instance for user authentication and authorization. 

The {ProductShortName} operator manages the {rhbk-short} instance and configures a dedicated link:https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.0/html/server_administration_guide/configuring-realms[realm] with necessary roles and permissions.

{ProductShortName}-managed {rhbk-short} instance allows you to perform advanced {rhbk-short} configurations, such as link:https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.0/html/server_administration_guide/user-storage-federation#adding_a_provider[adding a provider for User Federation] or link:https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.0/html/server_administration_guide/identity_broker[integrating identity providers]. To access the link:hhttps://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.0/html/server_administration_guide/configuring-realms#using_the_admin_console[{rhbk-short} Admin Console], enter the URL https://<_route_>/auth/admin in your browser by replacing < _route_ > with the {ProductShortName} web console address.

Example:

* MTA web console: https://mta-openshiftmta.example.com/
* {rhbk-short} Admin console: https://mta-openshiftmta.example.com/auth/admin

The admin credentials for {rhbk-short} are stored in a secret file named `mta-keycloak-rhbk` in the namespace where {ProductShortName} is installed.

To retrieve your admin credentials, run the following command:
[source,terminal]
----
$ oc get secret mta-keycloak-rhbk -n openshift-mta -o json| jq -r '.data.password | @base64d'
----

//To create a dedicated route for the {rhbk-short} instance, set the `rhsso_external_access` parameter to `true` in the Tackle custom resource (CR) for {ProductShortName}. #QE asked to remove this line.

//include::analyzer-rbac-snippet.adoc[]

.Additional resources
* link:https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.0/html/server_administration_guide/user-storage-federation#ldap[Configuring LDAP and Active Directory in {rhbk-short}]
* link:https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.0/html/server_administration_guide/red_hat_build_of_keycloak_features_and_concepts[Red Hat Build of Keycloak features and concepts]

[id="mta-roles-personas-users-permissions_{context}"]
=== Roles, Personas, Users, and Permissions

{ProductShortName} makes use of three roles, each of which corresponds to a persona:

.Roles and personas
[cols="50%,50%", options="header"]
|====
|Role
|Persona

|`tackle-admin`
|Administrator

|`tackle-architect`
|Architect

|`tackle-migrator`
|Migrator
|====

The roles are already defined in your {rhbk-short} instance. You do not need to create them.

If you are an {ProductShortName} administrator, you can create users in your {rhbk-short} and assign each user one or more roles, one role per persona.

[id="mta-roles-personas-ui-views_{context}"]
==== Roles, personas, and access to {WebName} views

Although a user can have more than one role, each role corresponds to a specific persona:

* Administrator: An administrator has all the permissions that architects and migrators have, along with the ability to create some application-wide configuration parameters that other users can consume but cannot change or view. Examples: Git credentials, Maven `settings.xml` files.

* Architect: A technical lead for the migration project who can run assessments and can create and modify applications and information related to them. An architect cannot modify or delete sensitive information, but can consume it. Example: Associate an existing credential to the repository of a specific application.

* Migrator: A user who can analyze applications, but not create, modify, or delete them.

As described in xref:mta-ui-interface-views[User interface views], {ProductShortName} has two views, *Administration* and *Migration*.

Only administrators can access *Administration* view. Architects and migrators have no access to *Administration* view, they cannot even see it.

Administrators can perform all actions supported by *Migration* view. Architects and migrators can see all elements of *Migration* view, but their ability to perform actions in *Migration* view depends on the permissions granted to their role.

The ability of administrators, architects, and migrators to access the *Administration* and *Migration* views of the {ProductShortName} {WebName} is summarized in the table below:

.Roles vs. access to {ProductShortName} views
[cols=",,,",options="header",]
|===
|Menu
|Architect
|Migrator
|Admin
|Administration
|No
|No
|Yes
|Migration
|Yes
|Yes
|Yes
|===

[id="mta-roles-permissions_{context}"]
==== Roles and permissions

The following table contains the roles and permissions (scopes) that {ProductShortName} seeds the managed {rhbk-short} instance with:

[width="100%",cols="34%,33%,33%",]
|====
|*tackle-admin* |*Resource Name* |*Verbs*
| |addons |delete +
get +
post +
put +
| |adoptionplans |post +
get +
post +
put +
| |applications |delete +
get +
post +
put +
| |applications.facts |delete +
get +
post +
put +
| |applications.tags |delete +
get +
post +
put +
| |applications.bucket |delete +
get +
post +
put +
| |assessments |delete +
get +
patch +
post +
put +
| |businessservices |delete +
get +
post +
put +
| |dependencies |delete +
get +
post +
put +
| |identities |delete +
get +
post +
put +
| |imports |delete +
get +
post +
put +
| |jobfunctions |delete +
get +
post +
put +
| |proxies |delete +
get +
post +
put +
| |reviews |delete +
get +
post +
put +
| |settings |delete +
get +
post +
put +
| |stakeholdergroups |delete +
get +
post +
put +
| |stakeholders |delete +
get +
post +
put +
| |tags |delete +
get +
post +
put +
| |tagtypes |delete +
get +
post +
put +
| |tasks |delete +
get +
post +
put +
| |tasks.bucket |delete +
get +
post +
put +
| |tickets |delete +
get +
post +
put +
| |trackers |delete +
get +
post +
put +
| |cache |delete +
get +
| |files |delete +
get +
post +
put +
| |rulebundles |delete +
get +
post +
put +
|====

[width="100%",cols="34%,33%,33%",]
|===
|*tackle-architect* | *Resource Name* |*Verbs*
| |addons |delete +
get +
post +
put +
| |applications.bucket |delete +
get +
post +
put +
| |adoptionplans |post +
| |applications |delete +
get +
post +
put +
| |applications.facts |delete +
get +
post +
put +
| |applications.tags |delete +
get +
post +
put +
| |assessments |delete +
get +
patch +
post +
put +
| |businessservices |delete +
get +
post +
put +
| |dependencies |delete +
get +
post +
put +
| |identities |get +
| |imports |delete +
get +
post +
put +
| |jobfunctions |delete +
get +
post +
put +
| |proxies |get +
| |reviews |delete +
get +
post +
put +
| |settings |get +
| |stakeholdergroups |delete +
get +
post +
put +
| |stakeholders |delete +
get +
post +
put +
| |tags |delete +
get +
post +
put +
| |tagtypes |delete +
get +
post +
put +
| |tasks |delete +
get +
post +
put +
| |tasks.bucket |delete +
get +
post +
put +
| |trackers |get +
| |tickets |delete +
get +
post +
put +
| |cache |get +
| |files |delete +
get +
post +
put +
| |rulebundles |delete +
get +
post +
put +
|===

[width="100%",cols="34%,33%,33%",]
|===
|*tackle-migrator* | *Resource Name* |*Verbs*
| |addons |get +
| |adoptionplans |post +
| |applications |get +
| |applications.facts |get +
| |applications.tags |get +
| |applications.bucket |get +
| |assessments |get +
post +
| |businessservices |get +
| |dependencies |delete +
get +
post +
put +
| |identities |get +
| |imports |get +
| |jobfunctions |get +
| |proxies |get +
| |reviews |get +
post +
put +
| |settings |get +
| |stakeholdergroups |get +
| |stakeholders |get +
| |tags |get +
| |tagtypes |get +
| |tasks |delete +
get +
post +
put +
| |tasks.bucket |delete +
get +
post +
put +
| |tackers |get +
| |tickets |get +
| |cache |get +
| |files |get +
| |rulebundles |get +
|===
